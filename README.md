# ğŸ§  Multimodal LLMs - Medical Help App using GPT-4V

## ğŸ“Œ Overview
This project leverages **Multimodal Large Language Models (LLMs)**, specifically **GPT-4V**, to build an AI-powered **Medical Help Application**. It aims to enhance healthcare accessibility by integrating **text and image-based medical assistance**.

## ğŸš€ Features
- ğŸ¥ **AI-Powered Medical Assistance** - Provides insights based on symptoms and queries.
- ğŸ–¼ï¸ **Image-Based Diagnosis** - Users can upload images (e.g., skin conditions) for preliminary AI analysis.
- ğŸ“ **Prescription & Treatment Suggestions** - AI-driven suggestions for common health issues.
- ğŸ” **Medical Report Analysis** - Extracts insights from medical reports.
- ğŸ™ï¸ **Voice & Text-Based Interaction** - Supports multimodal communication for better accessibility.

## ğŸ—ï¸ Tech Stack
- **Backend:** Python, Flask
- **LLM Framework:** GPT-4V
- **Frontend:** Streamlit / React (optional)
- **Deployment:** Docker, FastAPI
- **Data Processing:** OpenCV, Pandas
- **Authentication:** OAuth2 (for user security)


## ğŸ› ï¸ Installation & Setup
1. **Clone the Repository**
   ```sh
   git clone https://github.com/RakeshthakurIITian/Multimodel_llms.git
   cd Multimodel_llms
2. python -m venv multimodel_llms
source multimodel_llms/bin/activate  # macOS/Linux
multimodel_llms\Scripts\activate     # Windows
3. pip install -r requirements.txt
4. python app.py


