# 🧠 Multimodal LLMs - Medical Help App using GPT-4V

## 📌 Overview
This project leverages **Multimodal Large Language Models (LLMs)**, specifically **GPT-4V**, to build an AI-powered **Medical Help Application**. It aims to enhance healthcare accessibility by integrating **text and image-based medical assistance**.

## 🚀 Features
- 🏥 **AI-Powered Medical Assistance** - Provides insights based on symptoms and queries.
- 🖼️ **Image-Based Diagnosis** - Users can upload images (e.g., skin conditions) for preliminary AI analysis.
- 📝 **Prescription & Treatment Suggestions** - AI-driven suggestions for common health issues.
- 🔍 **Medical Report Analysis** - Extracts insights from medical reports.
- 🎙️ **Voice & Text-Based Interaction** - Supports multimodal communication for better accessibility.

## 🏗️ Tech Stack
- **Backend:** Python, Flask
- **LLM Framework:** GPT-4V
- **Frontend:** Streamlit / React (optional)
- **Deployment:** Docker, FastAPI
- **Data Processing:** OpenCV, Pandas
- **Authentication:** OAuth2 (for user security)


## 🛠️ Installation & Setup
1. **Clone the Repository**
   ```sh
   git clone https://github.com/RakeshthakurIITian/Multimodel_llms.git
   cd Multimodel_llms
2. python -m venv multimodel_llms
source multimodel_llms/bin/activate  # macOS/Linux
multimodel_llms\Scripts\activate     # Windows
3. pip install -r requirements.txt
4. python app.py


